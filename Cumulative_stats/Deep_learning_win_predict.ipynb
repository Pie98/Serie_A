{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pandasql as ps\n",
    "import os\n",
    "import random \n",
    "import numpy as np\n",
    "import warnings\n",
    "import tensorflow as tf \n",
    "from preprocess_days_stats import preprocess_cumulative_stats, preprocess_match_days\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from preprocess_features import preprocess_columns, preprocess_columns_with_odds\n",
    "from tensorflow.keras import layers\n",
    "from helper_functions_tensorflow import CSVLoggerCallback, CSVLoggerCallbackParams, plot_loss_curve\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Ignora tutti i warning temporaneamente\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "df_giornate = preprocess_match_days(r\"c:\\Users\\Hp\\Documents\\Serie_A_dump\\csv_serie_a\")\n",
    "df_giornate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capiamo l'accuracy da battere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_1 = [\n",
    "      ( (df_giornate[f'home_win_odds'] <= df_giornate['draw_odds']) & (df_giornate[f'home_win_odds'] <= df_giornate[f'away_win_odds'] )), \n",
    "      ( (df_giornate['draw_odds'] <= df_giornate[f'away_win_odds']) & (df_giornate['draw_odds'] <= df_giornate[f'home_win_odds'] )), \n",
    "      ( (df_giornate[f'away_win_odds'] <= df_giornate['draw_odds']) & (df_giornate[f'away_win_odds'] <= df_giornate[f'home_win_odds'] ))\n",
    "      ]\n",
    "values=['H','D','A']\n",
    "\n",
    "predizioni_bookmakers =np.select(conditions_1, values)\n",
    "risultati_effettivi = np.array(df_giornate['ft_result'])\n",
    "round(accuracy_score(predizioni_bookmakers, risultati_effettivi),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l'accuracy da battere è 54.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salviamo i risultati della baseline in un csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Nome del file CSV\n",
    "nome_file = \"results/baseline_KNN.csv\"\n",
    "\n",
    "# Apri il file CSV in modalità scrittura\n",
    "with open(nome_file, mode='w', newline='') as file_csv:\n",
    "    # Creazione dell'oggetto writer\n",
    "    writer = csv.writer(file_csv)\n",
    "\n",
    "    # Scrivi l'intestazione nel file CSV\n",
    "    writer.writerow(['giorni_cumulativi', 'vanumero_colonne', 'oversample', 'vicini', 'val_accuracy', 'train_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from preprocess_features import preprocess_columns\n",
    "\n",
    "for giorni_cumulativi in range(10):\n",
    "    giorni_cumulativi = giorni_cumulativi+1\n",
    "    df_Serie_A_days, stats_teams_serie_A = preprocess_cumulative_stats(dataframe=df_giornate, giorni_cumulativi=giorni_cumulativi)\n",
    "    for numero_colonne in ['all','less','few']:    \n",
    "        for oversample in [True, False]:\n",
    "            (X_train_norm, X_valid_norm, X_test_norm, Train_labels_encoded, Valid_labels_encoded, Test_labels_encoded, \n",
    "               X_train_df, X_valid_df, X_test_df, Train_labels, Valid_labels, Test_labels) = preprocess_columns(\n",
    "                df_Serie_A_days,numero_colonne,giorni_cumulativi, oversample=oversample)\n",
    "\n",
    "            best_accuracy=0\n",
    "            for vicini in range(3,40): #prendiamo diversi numeri di vicini possibili\n",
    "                knn_model = KNeighborsClassifier(n_neighbors=3*vicini) #addestriamo il modello con questo numero di vicini \n",
    "                knn_model.fit(X_train_norm, Train_labels_encoded) \n",
    "                y_valid = knn_model.predict(X_valid_norm)\n",
    "                val_accuracy = round(accuracy_score(Valid_labels_encoded, y_valid),2)\n",
    "                y_train = knn_model.predict(X_train_norm)\n",
    "                train_accuracy = round(accuracy_score(Train_labels_encoded, y_train),2)\n",
    "\n",
    "                if val_accuracy > best_accuracy:\n",
    "                    best_accuracy = val_accuracy\n",
    "                    \n",
    "                    # Scrivi i dati nel file CSV\n",
    "                    with open(nome_file, mode='a', newline='') as file_csv:\n",
    "                        writer = csv.writer(file_csv)\n",
    "                        writer.writerow([giorni_cumulativi, numero_colonne, oversample, vicini*3, val_accuracy, train_accuracy])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_KNN_df = pd.read_csv(r'results/baseline_KNN.csv').sort_values(['oversample','val_accuracy'],ascending=[False,False])\n",
    "baseline_KNN_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_KNN_df = pd.read_csv(r'results/baseline_KNN.csv').sort_values(['oversample','val_accuracy'],ascending=[True,False])\n",
    "baseline_KNN_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# Create a function to implement a ModelCheckpoint callback with a specific filename \n",
    "def create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n",
    "  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name), # create filepath to save model\n",
    "                                            verbose=0, # only output a limited amount of text\n",
    "                                            monitor='val_loss',\n",
    "                                            save_best_only=True) # save only the best model to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniziamo con poche features e 3 giorni cumulativi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_compile_dense(num_dense_layers, first_dropout, other_dropouts, first_num_neurons,\n",
    "                              other_num_neurons, input_shape, first_activation='relu', other_activations='relu'):\n",
    "\n",
    "    inputs = layers.Input(shape=(input_shape,))\n",
    "    x = layers.Dense(first_num_neurons, activation=first_activation)(inputs)\n",
    "    x = layers.Dropout(first_dropout)(x)  # Aggiunto il layer di dropout per ridurre overfitting\n",
    "    for i in range(num_dense_layers-1):\n",
    "        x = layers.Dense(other_num_neurons, activation=other_activations)(x)\n",
    "        x = layers.Dropout(other_dropouts)(x) \n",
    "    outputs = layers.Dense(3, activation='softmax')(x)\n",
    "    dense_model = tf.keras.Model(inputs, outputs, name='model_1_dense')\n",
    "\n",
    "    # compilo il modello \n",
    "    dense_model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics = 'accuracy',\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "    )\n",
    "\n",
    "    return dense_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for giorni_cumulativi in [5]:\n",
    "    # calcolo il dataframe con i giorni cumulativi \n",
    "    df_Serie_A_days, stats_teams_serie_A = preprocess_cumulative_stats(dataframe=df_giornate, giorni_cumulativi=giorni_cumulativi)\n",
    "    print(f'giorni cumulativi: {giorni_cumulativi}')\n",
    "    # seleziono il numero di features da usare nel modello \n",
    "    for numero_colonne in ['all','less','few']:\n",
    "        print(f'numero colonne: {numero_colonne}')    \n",
    "        X_train_norm, X_valid_norm, X_test_norm, Train_labels_encoded, Valid_labels_encoded, Test_labels_encoded = preprocess_columns(\n",
    "            df_Serie_A_days,numero_colonne,giorni_cumulativi)\n",
    "        input_shape = X_train_norm.shape[1]\n",
    "        # fast preprocessing \n",
    "        X_train_norm = tf.data.Dataset.from_tensor_slices((X_train_norm, Train_labels_encoded))\n",
    "        X_valid_norm = tf.data.Dataset.from_tensor_slices((X_valid_norm, Valid_labels_encoded))\n",
    "        X_train_norm = X_train_norm.batch(32).prefetch(tf.data.AUTOTUNE) #Autotune è per dirgli di prefetchare tanti dati quanti può\n",
    "        X_valid_norm = X_valid_norm.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "        #iniziamo a creare i modelli \n",
    "        for num_dense_layers in [2,3,4]:\n",
    "            for first_dropout in [0.0, 0.2, 0.4, 0.6]:\n",
    "                for other_dropouts in [0.0, 0.2, 0.4, 0.6]:\n",
    "                    for first_num_neurons in [32,64,128]:\n",
    "                        for other_num_neurons in [8, 16, 32]:\n",
    "                            other_activations = 'relu'\n",
    "                            first_activation = 'relu'\n",
    "                            model_1 = create_and_compile_dense(num_dense_layers, first_dropout, other_dropouts, first_num_neurons,\n",
    "                                    other_num_neurons, input_shape, first_activation, other_activations)\n",
    "\n",
    "                                # fitto il modello \n",
    "                            model_1.fit(\n",
    "                                    x = X_train_norm,\n",
    "                                    epochs = 15,\n",
    "                                    verbose=0,\n",
    "                                    validation_data = X_valid_norm,\n",
    "                                    callbacks = [\n",
    "                                        CSVLoggerCallbackParams('results.csv','model_1_dense', num_dense_layers, first_dropout, other_dropouts, first_num_neurons,\n",
    "                                                                    other_num_neurons, first_activation, other_activations), \n",
    "                                        #create_model_checkpoint(model_name= 'Dense_model_best')\n",
    "                                                ] \n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_5_days = pd.read_csv(r'C:\\Users\\Hp\\Serie_A\\results_5_days.csv', header=None)\n",
    "df_results_1_2_4 = pd.read_csv(r'C:\\Users\\Hp\\Serie_A\\results_1_2_4.csv', header=None)\n",
    "df_results =pd.concat([df_results_5_days,df_results_1_2_4],ignore_index=True)\n",
    "df_results.columns = ['experiment', 'num_dense_layers', 'first_dropout', 'other_dropouts', 'first_num_neurons',\n",
    "                                    'other_num_neurons', 'first_activation', 'other_activations', 'epoch',\n",
    "                                    'loss', 'accuracy','val_loss','val_accuracy']\n",
    "df_results.sort_values(['val_loss'], inplace=True)\n",
    "df_results.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### il modello denso migliore "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "giorni =5, colonne = less, val_loss: 0.9921 - val_accuracy: 0.5247\n",
    "\n",
    "giorni = 2, colonne = few, val_loss: 0.9895 - val_accuracy: 0.5220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(12)\n",
    "\n",
    "giorni_cumulativi = 2\n",
    "# calcolo il dataframe con i giorni cumulativi \n",
    "df_Serie_A_days, stats_teams_serie_A = preprocess_cumulative_stats(dataframe=df_giornate, giorni_cumulativi=giorni_cumulativi)\n",
    "\n",
    "#scelgo le colonne che voglio usare\n",
    "numero_colonne = 'less'\n",
    "print(f'numero colonne: {numero_colonne}')    \n",
    "(X_train_norm, X_valid_norm, X_test_norm, Train_labels_encoded, Valid_labels_encoded, Test_labels_encoded, \n",
    "               X_train_df, X_valid_df, X_test_df, Train_labels, Valid_labels, Test_labels) = preprocess_columns(\n",
    "                                                                                        df_Serie_A_days,numero_colonne,giorni_cumulativi, oversample=False)\n",
    "input_shape = X_train_norm.shape[1]\n",
    "Dataset_train_norm = tf.data.Dataset.from_tensor_slices((X_train_norm))\n",
    "Train_labels_encoded = tf.data.Dataset.from_tensor_slices(Train_labels_encoded) # make labels\n",
    "Dataset_train_norm = tf.data.Dataset.zip((Dataset_train_norm, Train_labels_encoded))\n",
    "\n",
    "Dataset_valid_norm = tf.data.Dataset.from_tensor_slices((X_valid_norm))\n",
    "Valid_labels_encoded = tf.data.Dataset.from_tensor_slices(Valid_labels_encoded) # make labels\n",
    "Dataset_valid_norm = tf.data.Dataset.zip((Dataset_valid_norm, Valid_labels_encoded))\n",
    "\n",
    "Dataset_test_norm = tf.data.Dataset.from_tensor_slices((X_test_norm))\n",
    "Test_labels_encoded = tf.data.Dataset.from_tensor_slices(Test_labels_encoded) # make labels\n",
    "Dataset_test_norm = tf.data.Dataset.zip((Dataset_test_norm, Test_labels_encoded))\n",
    "\n",
    "Dataset_train_norm = Dataset_train_norm.batch(32).prefetch(tf.data.AUTOTUNE) #Autotune è per dirgli di prefetchare tanti dati quanti può\n",
    "Dataset_valid_norm = Dataset_valid_norm.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "Dataset_test_norm = Dataset_test_norm.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#creo e compilo il modello \n",
    "model_1 = create_and_compile_dense(num_dense_layers=3, first_dropout=0.2, other_dropouts=0.1, first_num_neurons=32,\n",
    "                            other_num_neurons=8, input_shape=input_shape, first_activation='relu', other_activations='relu')\n",
    "\n",
    "# fitto il modello \n",
    "history_1 = model_1.fit(\n",
    "                x = Dataset_train_norm,\n",
    "                epochs = 15,\n",
    "                verbose = 1,\n",
    "                batch_size = 128,\n",
    "                validation_data = Dataset_valid_norm,\n",
    "                callbacks = [\n",
    "                                #CSVLoggerCallbackParams('results.csv','model_1_dense', num_dense_layers, first_dropout, other_dropouts, first_num_neurons,\n",
    "                                #                other_num_neurons, first_activation, other_activations), \n",
    "                                #create_model_checkpoint(model_name= 'Dense_model_best') \n",
    "                            ] \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAlutiamo il modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = tf.keras.models.load_model(r'C:\\Users\\Hp\\Serie_A\\model_experiments\\Dense_model_best')\n",
    "model_1.evaluate(Dataset_valid_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curve(history_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nota un po' di overfitting nel nostro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.merge(X_valid_df, df_giornate, how='inner', on=['stagione', 'awayteam','hometeam']).reset_index()\n",
    "len(result_df), len(result_df['stagione'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo un po' di risultati \n",
    "model_1_pred_probs = model_1.predict(X_valid_norm)\n",
    "model_1_prob = model_1.predict(X_valid_norm).max(axis=1)\n",
    "model_1_predictions = model_1_pred_probs.argmax(axis=1)\n",
    "model_1_compare = pd.DataFrame({\n",
    "                                'stagione': list( result_df['stagione'] ),\n",
    "                                'hometeam': list( result_df['hometeam'] ),\n",
    "                                'awayteam': list( result_df['awayteam'] ),\n",
    "                                'preds': model_1_predictions, \n",
    "                                'result': Valid_labels, \n",
    "                                'pred_prob': model_1_prob,\n",
    "                                'home_win_odds': list( result_df['home_win_odds'] ),\n",
    "                                'draw_odds': list( result_df['draw_odds'] ),\n",
    "                                'away_win_odds': list( result_df['away_win_odds'])\n",
    "                                })\n",
    "model_1_compare['bookmakers_pred'] = np.argmin(result_df[['home_win_odds', 'draw_odds', 'away_win_odds']].fillna(0.0).to_numpy(), axis=1)\n",
    "model_1_compare['bookmakers_prob'] = np.nanmin(result_df[['home_win_odds', 'draw_odds', 'away_win_odds']].fillna(0.0).to_numpy(), axis=1)\n",
    "\n",
    "# Assegno ai valori encoded dei valori più comprensibili per vittoria pareggio sconfitta\n",
    "conditions = [\n",
    "(model_1_compare['preds'] == 2),  # Condizione per Home Win\n",
    "(model_1_compare['preds'] == 1),  # Condizione per Away Win\n",
    "(model_1_compare['preds'] == 0)   # Condizione per Draw\n",
    "]\n",
    "conditions_bookmakers = [\n",
    "(model_1_compare['bookmakers_pred'] == 0),  # Condizione per Home Win\n",
    "(model_1_compare['bookmakers_pred'] == 2),  # Condizione per Away Win\n",
    "(model_1_compare['bookmakers_pred'] == 1)   # Condizione per Draw\n",
    "]\n",
    "\n",
    "# Valori corrispondenti alle condizioni\n",
    "values = ['W', 'L', 'D']\n",
    "\n",
    "# Creazione della nuova colonna 'result' e 'points\n",
    "model_1_compare['preds'] = np.select(conditions, values)\n",
    "model_1_compare['bookmakers_pred'] = np.select(conditions_bookmakers, values)\n",
    "model_1_compare['is_correct'] = model_1_compare['preds'] == model_1_compare['result']\n",
    "\n",
    "len(model_1_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i risultati più regenti \n",
    "model_1_compare.sort_values(['stagione'], ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i nostri riusltati corretti e quelli sbagliati della bookmakers \n",
    "model_1_compare[(model_1_compare['is_correct']) & (model_1_compare['result']!=model_1_compare['bookmakers_pred'])].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizziamo i risultati più sbagliati\n",
    "model_1_compare[(~model_1_compare['is_correct'])].sort_values(['pred_prob'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vede che molti errori sono dati da squadre blasonate che giocano in casa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONV1D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_compile_conv_1d(num_conv_layers, first_dropout, other_dropouts, first_filters_num, first_kernel_size,\n",
    "                               other_num_filters, other_kernel_sizem, input_shape, dense_neurons, first_activation='relu', other_activations='relu'):\n",
    "    inputs = layers.Input(shape=(input_shape,))\n",
    "    x = layers.Reshape((input_shape, 1))(inputs) # add an extra dimension for timesteps\n",
    "    x = layers.Conv1D(filters=first_filters_num, kernel_size=first_kernel_size, activation=first_activation)(x)\n",
    "    x = layers.Dropout(first_dropout)(x) \n",
    "    for i in range(num_conv_layers-1):\n",
    "        x = layers.Conv1D(filters=other_num_filters, kernel_size=other_kernel_sizem, activation=first_activation)(x)\n",
    "        x = layers.Dropout(other_dropouts)(x) \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(dense_neurons)(x)\n",
    "    x = layers.Dropout(other_dropouts)(x) \n",
    "    outputs = layers.Dense(3, activation='softmax')(x)\n",
    "    conv_model = tf.keras.Model(inputs, outputs, name='model_1_dense')\n",
    "\n",
    "    # compilo il modello \n",
    "    conv_model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics = 'accuracy',\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "    )\n",
    "\n",
    "    return conv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(12)\n",
    "\n",
    "giorni_cumulativi = 3\n",
    "# calcolo il dataframe con i giorni cumulativi \n",
    "df_Serie_A_days, stats_teams_serie_A = preprocess_cumulative_stats(dataframe=df_giornate, giorni_cumulativi=giorni_cumulativi)\n",
    "\n",
    "#scelgo le colonne che voglio usare\n",
    "numero_colonne = 'less'\n",
    "print(f'numero colonne: {numero_colonne}')    \n",
    "(X_train_norm, X_valid_norm, X_test_norm, Train_labels_encoded, Valid_labels_encoded, Test_labels_encoded, \n",
    "               X_train_df, X_valid_df, X_test_df, Train_labels, Valid_labels, Test_labels) = preprocess_columns(\n",
    "                                                                                        df_Serie_A_days,numero_colonne,giorni_cumulativi, oversample=False)\n",
    "input_shape = X_train_norm.shape[1]\n",
    "\n",
    "# fast preprocessing \n",
    "Dataset_train_norm = tf.data.Dataset.from_tensor_slices((X_train_norm, Train_labels_encoded))\n",
    "Dataset_valid_norm = tf.data.Dataset.from_tensor_slices((X_valid_norm, Valid_labels_encoded))\n",
    "Dataset_train_norm = Dataset_train_norm.batch(32).prefetch(tf.data.AUTOTUNE) #Autotune è per dirgli di prefetchare tanti dati quanti può\n",
    "Dataset_valid_norm = Dataset_valid_norm.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#creo e compilo il modello \n",
    "model_2 = create_and_compile_conv_1d(num_dense_layers=3, first_dropout=0.2, other_dropouts=0.0,\n",
    "                            other_num_neurons=8, input_shape=input_shape, first_activation='relu', other_activations='relu')\n",
    "\n",
    "# fitto il modello \n",
    "history_2 = model_2.fit(\n",
    "                x = Dataset_train_norm,\n",
    "                epochs = 15,\n",
    "                verbose = 1,\n",
    "                validation_data = Dataset_valid_norm,\n",
    "                callbacks = [\n",
    "                                #CSVLoggerCallbackParams('results.csv','model_1_dense', num_dense_layers, first_dropout, other_dropouts, first_num_neurons,\n",
    "                                #                other_num_neurons, first_activation, other_activations), \n",
    "                                #create_model_checkpoint(model_name= 'CONV1D_model_best') \n",
    "                            ] \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unisco il modello dense che ho creato sopra con uno che tenga conto delle quote "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to implement a ModelCheckpoint callback with a specific filename \n",
    "def create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n",
    "  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name), # create filepath to save model\n",
    "                                            verbose=0, # only output a limited amount of text\n",
    "                                            monitor='val_loss',\n",
    "                                            save_best_only=True) # save only the best model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(12)\n",
    "\n",
    "giorni_cumulativi = 2\n",
    "# calcolo il dataframe con i giorni cumulativi \n",
    "df_Serie_A_days, stats_teams_serie_A = preprocess_cumulative_stats(dataframe=df_giornate, giorni_cumulativi=giorni_cumulativi)\n",
    "\n",
    "#scelgo le colonne che voglio usare\n",
    "numero_colonne = 'less'\n",
    "print(f'numero colonne: {numero_colonne}')    \n",
    "(X_train_norm, X_valid_norm, X_test_norm, Train_labels_encoded, Valid_labels_encoded, Test_labels_encoded, \n",
    "    X_train_df, X_valid_df, X_test_df, Train_labels, Valid_labels, Test_labels,Train_odds_df,Valid_odds_df,Test_odds_df) = preprocess_columns_with_odds(\n",
    "                                                                                        df_Serie_A_days,numero_colonne,giorni_cumulativi, oversample=False)\n",
    "input_shape = X_train_norm.shape[1]\n",
    "\n",
    "print(X_valid_norm.shape,X_test_norm.shape,len(Test_odds_df))\n",
    "\n",
    "# fast preprocessing \n",
    "Dataset_train_norm = tf.data.Dataset.from_tensor_slices((X_train_norm, Train_odds_df))\n",
    "Train_labels_encoded = tf.data.Dataset.from_tensor_slices(Train_labels_encoded) # make labels\n",
    "Dataset_train_norm = tf.data.Dataset.zip((Dataset_train_norm, Train_labels_encoded))\n",
    "\n",
    "Dataset_valid_norm = tf.data.Dataset.from_tensor_slices((X_valid_norm, Valid_odds_df))\n",
    "Valid_labels_encoded = tf.data.Dataset.from_tensor_slices(Valid_labels_encoded) # make labels\n",
    "Dataset_valid_norm = tf.data.Dataset.zip((Dataset_valid_norm, Valid_labels_encoded))\n",
    "\n",
    "Dataset_Test_norm = tf.data.Dataset.from_tensor_slices((X_test_norm, Test_odds_df))\n",
    "Test_labels_encoded = tf.data.Dataset.from_tensor_slices(Test_labels_encoded) # make labels\n",
    "Dataset_Test_norm = tf.data.Dataset.zip((Dataset_Test_norm, Test_labels_encoded))\n",
    "\n",
    "Dataset_train_norm = Dataset_train_norm.batch(32).prefetch(tf.data.AUTOTUNE) #Autotune è per dirgli di prefetchare tanti dati quanti può\n",
    "Dataset_valid_norm = Dataset_valid_norm.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "Dataset_Test_norm = Dataset_Test_norm.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(Dataset_valid_norm)\n",
    "Dataset_Test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(12)\n",
    "\n",
    "#creo e compilo il modello denso\n",
    "inputs = layers.Input(shape=(input_shape,))\n",
    "x = layers.Dense(64, activation='relu')(inputs)\n",
    "x = layers.Dropout(0.2)(x)  # Aggiunto il layer di dropout per ridurre overfitting\n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "x = layers.Dropout(0.1)(x) \n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "outputs = layers.Dropout(0.1)(x) \n",
    "dense_model = tf.keras.Model(inputs, outputs, name='model_3_dense')\n",
    "\n",
    "#creo un modello che interpreti le odds\n",
    "inputs_odds = layers.Input(shape=(3,))\n",
    "x = layers.Dense(8, activation='relu')(inputs_odds)\n",
    "outputs_odds = layers.Dense(3, activation='linear')(x)\n",
    "model_3_odds = tf.keras.Model(inputs_odds, outputs_odds, name='model_3_odds')\n",
    "\n",
    "#unisco i due modelli \n",
    "# 3. Concatenate token and char inputs \n",
    "odds_concat = layers.Concatenate(name=\"odds_dense_model\")([model_3_odds.output,\n",
    "                                                                  dense_model.output])\n",
    "x = layers.Dense(16, activation='tanh')(odds_concat)\n",
    "x =layers.Dropout(0.5)(x)\n",
    "output_layer = layers.Dense(3, activation = 'softmax')(x)\n",
    "\n",
    "#Costruisco il modello con char e token input\n",
    "model_3 =tf.keras.Model(\n",
    "    inputs=[dense_model.input, model_3_odds.input],\n",
    "    outputs=output_layer,\n",
    "    name='model_4_odds_concat'\n",
    ")\n",
    "\n",
    "# compilo il modello \n",
    "model_3.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics = 'accuracy',\n",
    "    optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    ")\n",
    "\n",
    "#Se vuoi ridurre il tasso di apprendimento del 10% ogni 10 epoche\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch == 5:\n",
    "        return lr * 0.01  # Riduci il tasso di apprendimento del 10% ogni 10 epoche\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(12)\n",
    "\n",
    "# fitto il modello \n",
    "history_3 = model_3.fit(\n",
    "                x = Dataset_train_norm,\n",
    "                epochs = 6,\n",
    "                verbose = 1,\n",
    "                batch_size = 32,\n",
    "                validation_data = Dataset_valid_norm,\n",
    "                callbacks = [\n",
    "                                CSVLoggerCallbackParams('results.csv','model_3_dense', giorni_cumulativi,numero_colonne, 3, 0.2, 0.0, 32,\n",
    "                                                8, 'relu', 'relu'), \n",
    "                                lr_scheduler,\n",
    "                                create_model_checkpoint(model_name= 'Dense_model_best_1') \n",
    "                            ] \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valuto il modello migliore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = tf.keras.models.load_model(r'C:\\Users\\Hp\\Serie_A\\model_experiments\\Dense_model_best_1')\n",
    "model_3.evaluate(Dataset_Test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curve(history_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vediamo come performano i nostri risultati rispetto a quelli della bookmakers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.merge(X_valid_df, df_giornate, how='inner', on=['stagione', 'awayteam','hometeam']).reset_index()\n",
    "\n",
    "# Visualizziamo un po' di risultati \n",
    "model_3_pred_probs = model_3.predict((X_valid_norm, Valid_odds_df))\n",
    "model_3_prob = model_3_pred_probs.max(axis=1)\n",
    "model_3_predictions = model_3_pred_probs.argmax(axis=1)\n",
    "model_3_compare = pd.DataFrame({\n",
    "                                'stagione': list( result_df['stagione'] ),\n",
    "                                'hometeam': list( result_df['hometeam'] ),\n",
    "                                'awayteam': list( result_df['awayteam'] ),\n",
    "                                'preds': model_3_predictions, \n",
    "                                'result': Valid_labels, \n",
    "                                'pred_prob': model_3_prob,\n",
    "                                'home_win_odds': list( result_df['home_win_odds'] ),\n",
    "                                'draw_odds': list( result_df['draw_odds'] ),\n",
    "                                'away_win_odds': list( result_df['away_win_odds'])\n",
    "                                })\n",
    "model_3_compare['bookmakers_pred'] = np.argmin(result_df[['home_win_odds', 'draw_odds', 'away_win_odds']].fillna(0.0).to_numpy(), axis=1)\n",
    "model_3_compare['bookmakers_prob'] = np.nanmin(result_df[['home_win_odds', 'draw_odds', 'away_win_odds']].fillna(0.0).to_numpy(), axis=1)\n",
    "\n",
    "# Assegno ai valori encoded dei valori più comprensibili per vittoria pareggio sconfitta\n",
    "conditions = [\n",
    "(model_3_compare['preds'] == 2),  # Condizione per Home Win\n",
    "(model_3_compare['preds'] == 1),  # Condizione per Away Win\n",
    "(model_3_compare['preds'] == 0)   # Condizione per Draw\n",
    "]\n",
    "conditions_bookmakers = [\n",
    "(model_3_compare['bookmakers_pred'] == 0),  # Condizione per Home Win\n",
    "(model_3_compare['bookmakers_pred'] == 2),  # Condizione per Away Win\n",
    "(model_3_compare['bookmakers_pred'] == 1)   # Condizione per Draw\n",
    "]\n",
    "\n",
    "# Valori corrispondenti alle condizioni\n",
    "values = ['W', 'L', 'D']\n",
    "\n",
    "# Creazione della nuova colonna 'result' e 'points\n",
    "model_3_compare['preds'] = np.select(conditions, values)\n",
    "model_3_compare['bookmakers_pred'] = np.select(conditions_bookmakers, values)\n",
    "model_3_compare['is_correct'] = model_3_compare['preds'] == model_3_compare['result']\n",
    "model_3_compare['money_won'] = model_3_compare['pred_prob']*model_3_compare['bookmakers_prob']\n",
    "\n",
    "len(model_3_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i nostri riusltati corretti e quelli sbagliati della bookmakers\n",
    "test = model_3_compare[(model_3_compare['is_correct']) & (model_3_compare['result']!=model_3_compare['bookmakers_pred'])].sort_values(['money_won'], ascending=False).head(100)\n",
    "print(f'il nostro modello ha performato meglio in {len(test)} su {len(model_3_compare)}') \n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IL modello sembra completamente incapace di prevedere i pareggi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_3_compare[(model_3_compare['result']=='D')]), len(model_3_compare[(model_3_compare['result']=='D')&(model_3_compare['is_correct'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vincita soldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vediamo in quanti casi avremmo vinto soldi \n",
    "test_1 = model_3_compare[(model_3_compare['is_correct']) & (model_3_compare['money_won']>1)].sort_values(['money_won'], ascending=False)\n",
    "correct_results = model_3_compare[(model_3_compare['is_correct'])]\n",
    "print(f'il nostro modello ha performato meglio in {len(test_1)} su {len(model_3_compare)} totali e {len(correct_results)} casi in cui la predizione è corretta ') \n",
    "test_1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vediamo in quanti casi avremmo perso soldi \n",
    "test_2 = model_3_compare[(model_3_compare['money_won']>1)]\n",
    "wrong_results = model_3_compare[(~model_3_compare['is_correct'])].sort_values(['money_won'], ascending=False)\n",
    "test_2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vediamo quanti soldi avrei vinto e quanti perso\n",
    "test_1['bookmakers_prob'].sum()-len(test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facciamo gli stessi test sul dataset di test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.merge(X_test_df, df_giornate, how='inner', on=['stagione', 'awayteam','hometeam']).reset_index()\n",
    "\n",
    "# Visualizziamo un po' di risultati \n",
    "model_3_pred_probs = model_3.predict((X_test_norm, Test_odds_df))\n",
    "model_3_prob = model_3_pred_probs.max(axis=1)\n",
    "model_3_predictions = model_3_pred_probs.argmax(axis=1)\n",
    "model_3_compare = pd.DataFrame({\n",
    "                                'stagione': list( result_df['stagione'] ),\n",
    "                                'hometeam': list( result_df['hometeam'] ),\n",
    "                                'awayteam': list( result_df['awayteam'] ),\n",
    "                                'preds': model_3_predictions, \n",
    "                                'result': Test_labels, \n",
    "                                'pred_prob': model_3_prob,\n",
    "                                'home_win_odds': list( result_df['home_win_odds'] ),\n",
    "                                'draw_odds': list( result_df['draw_odds'] ),\n",
    "                                'away_win_odds': list( result_df['away_win_odds'])\n",
    "                                })\n",
    "model_3_compare['bookmakers_pred'] = np.argmin(result_df[['home_win_odds', 'draw_odds', 'away_win_odds']].fillna(0.0).to_numpy(), axis=1)\n",
    "model_3_compare['bookmakers_prob'] = np.nanmin(result_df[['home_win_odds', 'draw_odds', 'away_win_odds']].fillna(0.0).to_numpy(), axis=1)\n",
    "\n",
    "# Assegno ai valori encoded dei valori più comprensibili per vittoria pareggio sconfitta\n",
    "conditions = [\n",
    "(model_3_compare['preds'] == 2),  # Condizione per Home Win\n",
    "(model_3_compare['preds'] == 1),  # Condizione per Away Win\n",
    "(model_3_compare['preds'] == 0)   # Condizione per Draw\n",
    "]\n",
    "conditions_bookmakers = [\n",
    "(model_3_compare['bookmakers_pred'] == 0),  # Condizione per Home Win\n",
    "(model_3_compare['bookmakers_pred'] == 2),  # Condizione per Away Win\n",
    "(model_3_compare['bookmakers_pred'] == 1)   # Condizione per Draw\n",
    "]\n",
    "\n",
    "# Valori corrispondenti alle condizioni\n",
    "values = ['W', 'L', 'D']\n",
    "\n",
    "# Creazione della nuova colonna 'result' e 'points\n",
    "model_3_compare['preds'] = np.select(conditions, values)\n",
    "model_3_compare['bookmakers_pred'] = np.select(conditions_bookmakers, values)\n",
    "model_3_compare['is_correct'] = model_3_compare['preds'] == model_3_compare['result']\n",
    "model_3_compare['money_won'] = model_3_compare['pred_prob']*model_3_compare['bookmakers_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i nostri riusltati corretti e quelli sbagliati della bookmakers\n",
    "test = model_3_compare[(model_3_compare['is_correct']) & (model_3_compare['result']!=model_3_compare['bookmakers_pred'])].sort_values(['money_won'], ascending=False).head(100)\n",
    "print(f'il nostro modello ha performato meglio in {len(test)} su {len(model_3_compare)}') \n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i nostri riusltati sbagliati e quelli corretti della bookmakers\n",
    "test = model_3_compare[(~model_3_compare['is_correct']) & (model_3_compare['result']==model_3_compare['bookmakers_pred'])].sort_values(['money_won'], ascending=False).head(100)\n",
    "print(f'il nostro modello ha performato peggio in {len(test)} su {len(model_3_compare)}') \n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vediamo in quanti casi avremmo vinto soldi \n",
    "test_1 = model_3_compare[(model_3_compare['is_correct']) & (model_3_compare['money_won']>1)].sort_values(['money_won'], ascending=False)\n",
    "correct_results = model_3_compare[(model_3_compare['is_correct'])]\n",
    "print(f'il nostro modello ha performato meglio in {len(test_1)} su {len(model_3_compare)} totali e {len(correct_results)} casi in cui la predizione è corretta ') \n",
    "test_1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vediamo in quanti casi avremmo perso soldi \n",
    "test_2 = model_3_compare[(model_3_compare['money_won']>1)]\n",
    "wrong_results = model_3_compare[(~model_3_compare['is_correct'])].sort_values(['money_won'], ascending=False)\n",
    "print(f'il nostro modello ha performato peggio in {len(test_2)} su {len(model_3_compare)} totali e {len(wrong_results)} casi in cui la predizione è sbagliata ') \n",
    "test_2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vediamo quanti soldi avrei vinto e quanti perso\n",
    "test_1['bookmakers_prob'].sum()-len(test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizziamo le nostre predizioni peggiori nel dataframe di test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_compare[(~model_3_compare['is_correct'])].sort_values('pred_prob',ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "il nostro modello ha problemi a predire i pareggi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_3_compare[(model_3_compare['result']=='D')]), len(model_3_compare[(model_3_compare['result']=='D')&(model_3_compare['is_correct'])])\n",
    "# non ne ha predetto correttamente neanche uno "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_compare[(model_3_compare['result']=='D')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(12)\n",
    "\n",
    "giorni_cumulativi = 2\n",
    "# calcolo il dataframe con i giorni cumulativi \n",
    "df_Serie_A_days, stats_teams_serie_A = preprocess_cumulative_stats(dataframe=df_giornate, giorni_cumulativi=giorni_cumulativi)\n",
    "\n",
    "#scelgo le colonne che voglio usare\n",
    "numero_colonne = 'less'\n",
    "print(f'numero colonne: {numero_colonne}')    \n",
    "(X_train_norm, X_valid_norm, X_test_norm, Train_labels_encoded, Valid_labels_encoded, Test_labels_encoded, \n",
    "               X_train_df, X_valid_df, X_test_df, Train_labels, Valid_labels, Test_labels) = preprocess_columns(\n",
    "                                                                                        df_Serie_A_days,numero_colonne,giorni_cumulativi, oversample=False)\n",
    "input_shape = X_train_norm.shape[1]\n",
    "Dataset_train_norm = tf.data.Dataset.from_tensor_slices((X_train_norm))\n",
    "Train_labels_encoded = tf.data.Dataset.from_tensor_slices(Train_labels_encoded) # make labels\n",
    "Dataset_train_norm = tf.data.Dataset.zip((Dataset_train_norm, Train_labels_encoded))\n",
    "\n",
    "Dataset_valid_norm = tf.data.Dataset.from_tensor_slices((X_valid_norm))\n",
    "Valid_labels_encoded = tf.data.Dataset.from_tensor_slices(Valid_labels_encoded) # make labels\n",
    "Dataset_valid_norm = tf.data.Dataset.zip((Dataset_valid_norm, Valid_labels_encoded))\n",
    "\n",
    "Dataset_test_norm = tf.data.Dataset.from_tensor_slices((X_test_norm))\n",
    "Test_labels_encoded = tf.data.Dataset.from_tensor_slices(Test_labels_encoded) # make labels\n",
    "Dataset_test_norm = tf.data.Dataset.zip((Dataset_test_norm, Test_labels_encoded))\n",
    "\n",
    "Dataset_train_norm = Dataset_train_norm.batch(32).prefetch(tf.data.AUTOTUNE) #Autotune è per dirgli di prefetchare tanti dati quanti può\n",
    "Dataset_valid_norm = Dataset_valid_norm.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "Dataset_test_norm = Dataset_test_norm.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creo e compilo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(12)\n",
    "\n",
    "#creo e compilo il modello denso\n",
    "inputs = layers.Input(shape=(input_shape,))\n",
    "x = layers.Dense(32, activation='relu')(inputs)\n",
    "x = layers.Dropout(0.2)(x)  # Aggiunto il layer di dropout per ridurre overfitting\n",
    "x = layers.Dense(8, activation='relu')(x)\n",
    "x = layers.Dropout(0.1)(x) \n",
    "x = layers.Dense(8, activation='relu')(x)\n",
    "x = layers.Dropout(0.1)(x) \n",
    "outputs = layers.Dense(3, activation = 'softmax')(x)\n",
    "dense_model = tf.keras.Model(inputs, outputs, name='model_dense')\n",
    "\n",
    "# compilo il modello \n",
    "dense_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics = 'accuracy',\n",
    "    optimizer = tf.keras.optimizers.SGD()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fitto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(12)\n",
    "\n",
    "# fitto il modello \n",
    "history_test = dense_model.fit(\n",
    "                x = Dataset_train_norm,\n",
    "                epochs = 80,\n",
    "                verbose = 1,\n",
    "                batch_size = 32,\n",
    "                validation_data = Dataset_valid_norm,\n",
    "                callbacks = [\n",
    "                                #lr_scheduler,\n",
    "                                #create_model_checkpoint(model_name= 'Dense_model_best_1') \n",
    "                            ] \n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
