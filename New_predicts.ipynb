{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pandasql as ps\n",
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import random \n",
    "import numpy as np\n",
    "import warnings\n",
    "import tensorflow as tf \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from preprocess_days_stats import preprocess_match_days\n",
    "from preprocess_time_serie import preprocess_teams, create_time_series_features\n",
    "from preprocess_time_series_features import preprocess_features_time_series, create_fast_preprocessing_ts, preprocess_features_time_series_odds, create_fast_preprocessing_ts_odds\n",
    "from helper_functions_tensorflow import CSVLoggerCallback, CSVLoggerCallbackParams\n",
    "\n",
    "# Ignora tutti i warning temporaneamente\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features_time_series_odds_preds(df_Serie_A, num_features, percentual_preds=0.95):\n",
    "\n",
    "    all_features = ['ft_goals','ft_goals_conceded','shots','shots_target', 'fouls_done','corners_obtained', 'yellows', 'reds']\n",
    "    less_features = ['ft_goals','ft_goals_conceded','shots', 'fouls_done','corners_obtained', 'reds']\n",
    "    few_features = ['ft_goals','ft_goals_conceded','shots', 'reds']\n",
    "\n",
    "    Train_df = df_Serie_A.iloc[:10]\n",
    "    Valid_df = df_Serie_A.iloc[:10]\n",
    "    Test_df = df_Serie_A.iloc[int(len(df_Serie_A)*percentual_preds):]\n",
    "\n",
    "    Train_labels = Train_df[['ft_result']]\n",
    "    Valid_labels = Valid_df[['ft_result']]\n",
    "    Test_labels = Test_df[['ft_result']]\n",
    "    \n",
    "    Train_odds = Train_df[['home_win_odds','draw_odds','away_win_odds']]\n",
    "    Valid_odds = Valid_df[['home_win_odds','draw_odds','away_win_odds']]\n",
    "    Test_odds = Test_df[['home_win_odds','draw_odds','away_win_odds']]\n",
    "\n",
    "    # preprocess Train dataframe\n",
    "    Train_teams = Train_df[['stagione','hometeam','awayteam']]\n",
    "\n",
    "    if num_features == 'all':\n",
    "        features = all_features\n",
    "        print('utilizzando tutte le features')\n",
    "    elif num_features == 'less':\n",
    "        print('utilizzando meno features')\n",
    "        features = less_features\n",
    "    else:\n",
    "        print('utilizzando poche features')\n",
    "        features=few_features\n",
    "\n",
    "    Train_dict_features={}\n",
    "\n",
    "    for feature in features:\n",
    "        Train_dict_features[feature] = pd.DataFrame({})\n",
    "        for colonna in Train_df.columns:\n",
    "            pattern = re.compile(rf'^home_{feature}_\\d+$')\n",
    "            if pattern.match(colonna):\n",
    "                Train_dict_features[feature][colonna]=Train_df[colonna]\n",
    "        for colonna in Train_df.columns:\n",
    "            pattern = re.compile(rf'^away_{feature}_\\d+$')\n",
    "            if pattern.match(colonna):\n",
    "                Train_dict_features[feature][colonna]=Train_df[colonna]\n",
    "\n",
    "    #preprocess valid dataframe\n",
    "    Valid_teams = Valid_df[['stagione','hometeam','awayteam']]\n",
    "\n",
    "\n",
    "    if num_features == 'all':\n",
    "        features = all_features\n",
    "        print('utilizzando tutte le features')\n",
    "    elif num_features == 'less':\n",
    "        print('utilizzando meno features')\n",
    "        features = less_features\n",
    "    else:\n",
    "        print('utilizzando poche features')\n",
    "        features=few_features\n",
    "\n",
    "    Valid_dict_features={}\n",
    "\n",
    "    for feature in features:\n",
    "        Valid_dict_features[feature] = pd.DataFrame({})\n",
    "        for colonna in Valid_df.columns:\n",
    "            pattern = re.compile(rf'^home_{feature}_\\d+$')\n",
    "            if pattern.match(colonna):\n",
    "                Valid_dict_features[feature][colonna]=Valid_df[colonna]\n",
    "        for colonna in Valid_df.columns:\n",
    "            pattern = re.compile(rf'^away_{feature}_\\d+$')\n",
    "            if pattern.match(colonna):\n",
    "                Valid_dict_features[feature][colonna]=Valid_df[colonna]\n",
    "\n",
    "    # preprocess test dataframe\n",
    "    Test_teams = Test_df[['stagione','hometeam','awayteam']]\n",
    "\n",
    "    if num_features == 'all':\n",
    "        features = all_features\n",
    "        print('utilizzando tutte le features')\n",
    "    elif num_features == 'less':\n",
    "        print('utilizzando meno features')\n",
    "        features = less_features\n",
    "    else:\n",
    "        print('utilizzando poche features')\n",
    "        features=few_features\n",
    "\n",
    "    Test_dict_features={}\n",
    "\n",
    "    for feature in features:\n",
    "        Test_dict_features[feature] = pd.DataFrame({})\n",
    "        for colonna in Test_df.columns:\n",
    "            pattern = re.compile(rf'^home_{feature}_\\d+$')\n",
    "            if pattern.match(colonna):\n",
    "                Test_dict_features[feature][colonna]=Test_df[colonna]\n",
    "        for colonna in Test_df.columns:\n",
    "            pattern = re.compile(rf'^away_{feature}_\\d+$')\n",
    "            if pattern.match(colonna):\n",
    "                Test_dict_features[feature][colonna]=Test_df[colonna]\n",
    "\n",
    "    #encoding teams\n",
    "    # load the  transformer\n",
    "    teams_transf = joblib.load('transformers/teams_transformer.pkl')\n",
    "\n",
    "    Train_teams_encoded = teams_transf.transform(Train_teams)\n",
    "    Valid_teams_encoded = teams_transf.transform(Valid_teams)\n",
    "    Test_teams_encoded = teams_transf.transform(Test_teams)\n",
    "\n",
    "    #encoding labels\n",
    "    # load the  transformer\n",
    "    ordinal_encoder = joblib.load('transformers/ordinal_encoder_transformer.pkl')\n",
    "\n",
    "    Train_labels_encoded = np.squeeze(ordinal_encoder.transform(np.array(Train_labels).reshape(-1, 1)))\n",
    "    Valid_labels_encoded = np.squeeze(ordinal_encoder.transform(np.array(Valid_labels).reshape(-1, 1)))\n",
    "    Test_labels_encoded = np.squeeze(ordinal_encoder.transform(np.array(Test_labels).reshape(-1, 1)))  \n",
    "\n",
    "    #encoding numerical features\n",
    "    Train_dict_features_norm = Train_dict_features.copy()\n",
    "    Valid_dict_features_norm = Valid_dict_features.copy()\n",
    "    Test_dict_features_norm = Test_dict_features.copy()\n",
    "\n",
    "    for feature in list(Train_dict_features.keys()):\n",
    "        feature_list = Train_dict_features_norm[feature].columns\n",
    "        numerical_transf = make_column_transformer(\n",
    "            (MinMaxScaler(), feature_list), \n",
    "            sparse_threshold=0  \n",
    "        )\n",
    "        numerical_transf.fit(Train_dict_features[feature])\n",
    "\n",
    "        # save the  transformer\n",
    "        if save_tranformer: \n",
    "         joblib.dump(numerical_transf, 'transformers/numerical_transformer.pkl')\n",
    "\n",
    "        Train_dict_features_norm[feature] = numerical_transf.transform(Train_dict_features_norm[feature])\n",
    "        Valid_dict_features_norm[feature] = numerical_transf.transform(Valid_dict_features_norm[feature])\n",
    "        Test_dict_features_norm[feature] = numerical_transf.transform(Test_dict_features_norm[feature])\n",
    "    \n",
    "    # Encoding odds\n",
    "    odds_transf = make_column_transformer(\n",
    "        (MinMaxScaler(), ['home_win_odds','draw_odds','away_win_odds']), \n",
    "        sparse_threshold=0  \n",
    "    )\n",
    "\n",
    "    odds_transf.fit(Train_odds)\n",
    "\n",
    "    # save the  transformer\n",
    "    if save_tranformer: \n",
    "         joblib.dump(odds_transf, 'transformers/odds_transformer.pkl')\n",
    "\n",
    "    Train_odds_norm = odds_transf.transform(Train_odds)\n",
    "    Valid_odds_norm = odds_transf.transform(Valid_odds)\n",
    "    Test_odds_norm = odds_transf.transform(Test_odds)\n",
    "\n",
    "    return (Train_teams_encoded, Valid_teams_encoded, Test_teams_encoded, Train_labels_encoded, Valid_labels_encoded, Test_labels_encoded, \n",
    "            Train_dict_features_norm, Valid_dict_features_norm, Test_dict_features_norm, Train_teams, Valid_teams, Test_teams, Train_labels, Valid_labels, Test_labels, \n",
    "            Train_dict_features, Valid_dict_features, Test_dict_features, Train_df, Valid_df, Test_df, \n",
    "            Train_odds_norm, Valid_odds_norm, Test_odds_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the new results to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leggi il file CSV\n",
    "df = pd.read_csv(r\"C:\\Users\\Hp\\Documents\\Serie_A_dump\\csv_predictions\\december_four.csv\", parse_dates=['Date'], dayfirst=True)\n",
    "\n",
    "# Crea un DataFrame con 10 righe di zeri\n",
    "new_rows = pd.DataFrame(0, index=range(10), columns=df.columns)\n",
    "\n",
    "today_date = '2023-12-10'\n",
    "\n",
    "# Assegna valori specifici alle colonne 'Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam' per ciascuna riga\n",
    "new_rows.loc[0, ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam']] = ['I1', f'{today_date}', '17:30', 'Juventus', 'Napoli']\n",
    "new_rows.loc[1, ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam']] = ['I1', f'{today_date}', '17:30', 'Verona', 'Lazio']\n",
    "new_rows.loc[2, ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam']] = ['I1', f'{today_date}', '17:30', 'Atalanta', 'Milan']\n",
    "new_rows.loc[3, ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam']] = ['I1', f'{today_date}', '17:30', 'Inter', 'Udinese']\n",
    "new_rows.loc[4, ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam']] = ['I1', f'{today_date}', '17:30', 'Frosinone', 'Torino']\n",
    "new_rows.loc[5, ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam']] = ['I1', f'{today_date}', '17:30', 'Monza', 'Genoa']\n",
    "new_rows.loc[6, ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam']] = ['I1', f'{today_date}', '17:30', 'Salernitana', 'Bologna']\n",
    "new_rows.loc[7, ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam']] = ['I1', f'{today_date}', '17:30', 'Roma', 'Fiorentina']\n",
    "new_rows.loc[8, ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam']] = ['I1', f'{today_date}', '17:30', 'Empoli', 'Lecce']\n",
    "new_rows.loc[9, ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam']] = ['I1', f'{today_date}', '17:30', 'Cagliari', 'Sassuolo']\n",
    "\n",
    "\n",
    "# Aggiungi le nuove righe al DataFrame esistente\n",
    "new_csv = pd.concat([df, new_rows], ignore_index=True)\n",
    "new_csv['Date'] = pd.to_datetime(new_csv['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# Salva il DataFrame aggiornato su un nuovo file CSV\n",
    "new_csv.to_csv(r\"C:\\Users\\Hp\\Serie_A\\csv_predictions\\december_four.csv\")\n",
    "\n",
    "len(new_csv['HomeTeam'].unique()), len(new_csv['AwayTeam'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: december_four.csv\n",
      "Reading file: I1 (0).csv\n",
      "Reading file: I1 (1).csv\n",
      "Reading file: I1 (10).csv\n",
      "Reading file: I1 (11).csv\n",
      "Reading file: I1 (12).csv\n",
      "Reading file: I1 (13).csv\n",
      "Reading file: I1 (14).csv\n",
      "Reading file: I1 (15).csv\n",
      "Reading file: I1 (16).csv\n",
      "Reading file: I1 (17).csv\n",
      "Reading file: I1 (2).csv\n",
      "Reading file: I1 (3).csv\n",
      "Reading file: I1 (4).csv\n",
      "Reading file: I1 (5).csv\n",
      "Reading file: I1 (6).csv\n",
      "Reading file: I1 (7).csv\n",
      "Reading file: I1 (8).csv\n",
      "Reading file: I1 (9).csv\n",
      "preprocessing finished!\n",
      "utilizzando meno features\n",
      "preprocess finished\n",
      "utilizzando meno features\n",
      "utilizzando meno features\n",
      "utilizzando meno features\n"
     ]
    }
   ],
   "source": [
    "#preprocess the data\n",
    "df_giornate = preprocess_match_days(r\"C:\\Users\\Hp\\Serie_A\\csv_predictions\")\n",
    "num_features = 'less'\n",
    "num_giornate = 4\n",
    "Statistiche_squadre_dict = preprocess_teams(dataframe = df_giornate)\n",
    "df_Serie_A = create_time_series_features(num_features, Statistiche_squadre_dict, df_giornate, num_giornate).dropna()\n",
    "\n",
    "(Train_teams_encoded, Valid_teams_encoded, Test_teams_encoded, Train_labels_encoded, Valid_labels_encoded, Test_labels_encoded, \n",
    "    Train_dict_features_norm, Valid_dict_features_norm, Test_dict_features_norm, Train_teams, Valid_teams, Test_teams, Train_labels, Valid_labels, \n",
    "    Test_labels, Train_dict_features, Valid_dict_features, Test_dict_features, Train_df, Valid_df, Test_df, \n",
    "    Train_odds_norm, Valid_odds_norm, Test_odds_norm) = preprocess_features_time_series_odds(df_Serie_A, num_features, random_state=False)\n",
    "\n",
    "feature_input_shape = Test_dict_features_norm[list(Test_dict_features_norm.keys())[0]].shape[1]\n",
    "Train_teams_shape = Test_teams_encoded.shape[1]\n",
    "\n",
    "Dataset_train_norm, Dataset_valid_norm, Dataset_test_norm = create_fast_preprocessing_ts_odds(Train_teams_encoded, Train_dict_features_norm, Train_labels_encoded,\n",
    "                                                                                         Valid_teams_encoded, Valid_dict_features_norm,\n",
    "                                                                    Valid_labels_encoded,Test_teams_encoded, Test_dict_features_norm, Test_labels_encoded, \n",
    "                                                                    Train_odds_norm, Valid_odds_norm, Test_odds_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the model \n",
    "odds_model = tf.keras.models.load_model(r'c:\\Users\\Hp\\Serie_A\\model_experiments\\model_odds_time_series')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\Hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\Hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\layers\\reshaping\\reshape.py\", line 118, in _fix_unknown_dimension\n        raise ValueError(msg)\n\n    ValueError: Exception encountered when calling layer 'reshape_8' (type Reshape).\n    \n    total size of new array must be unchanged, input_shape = [78], output_shape = [86, 1]\n    \n    Call arguments received by layer 'reshape_8' (type Reshape):\n      • inputs=tf.Tensor(shape=(None, 78), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Visualizziamo un po' di risultati \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_odds_new_pred_probs \u001b[38;5;241m=\u001b[39m \u001b[43modds_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDataset_test_norm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model_odds_new_prob \u001b[38;5;241m=\u001b[39m model_odds_new_pred_probs\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m model_odds_new_predictions \u001b[38;5;241m=\u001b[39m model_odds_new_pred_probs\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileinve9ehn.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\Hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\Hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\layers\\reshaping\\reshape.py\", line 118, in _fix_unknown_dimension\n        raise ValueError(msg)\n\n    ValueError: Exception encountered when calling layer 'reshape_8' (type Reshape).\n    \n    total size of new array must be unchanged, input_shape = [78], output_shape = [86, 1]\n    \n    Call arguments received by layer 'reshape_8' (type Reshape):\n      • inputs=tf.Tensor(shape=(None, 78), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## Visualizziamo un po' di risultati \n",
    "model_odds_new_pred_probs = odds_model.predict((Dataset_test_norm))\n",
    "model_odds_new_prob = model_odds_new_pred_probs.max(axis=1)\n",
    "model_odds_new_predictions = model_odds_new_pred_probs.argmax(axis=1)\n",
    "model_odds_new_compare = pd.DataFrame({\n",
    "                                'stagione': list( Test_df['stagione'] ),\n",
    "                                'hometeam': list( Test_df['hometeam'] ),\n",
    "                                'awayteam': list( Test_df['awayteam'] ),\n",
    "                                'preds': model_odds_new_predictions, \n",
    "                                'best_pred_prob': model_odds_new_prob,\n",
    "                                'home_win_odds': list( Test_df['home_win_odds'] ),\n",
    "                                'draw_odds': list( Test_df['draw_odds'] ),\n",
    "                                'away_win_odds': list( Test_df['away_win_odds'])\n",
    "                                })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
